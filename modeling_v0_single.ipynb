{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description:\n",
    "\n",
    "#### Goals:\n",
    "- Build a forecasting model to predict future web traffic for approximately 145,000 Wikipedia pages.\n",
    "- Investigate the effectiveness of LSTM neural networks in handling multiple time series for this prediction problem.\n",
    "\n",
    "#### Assumptions:\n",
    "- The model is expected to forecast web traffic for 7 days based on the historical traffic from last 90 days.\n",
    "\n",
    "#### Data and project inspiration:\n",
    "https://www.kaggle.com/competitions/web-traffic-time-series-forecasting/data\n",
    "\n",
    "#### Features:\n",
    "- Main feature:\n",
    "    - Traffic history (number of visits)\n",
    "- Time-related features:\n",
    "    - Day of the week\n",
    "    - Day of the year\n",
    "- Page-specific features:\n",
    "    - Median visits over the whole period\n",
    "    - Page attributes: project, access, agent\n",
    "\n",
    "Logarithmic Transformation and Normalization were applied to the number of visits and the median of visits.\n",
    "\n",
    "*For further details refer to exploration.ipynb*\n",
    "\n",
    "#### Model architecture:\n",
    "The model consists of:\n",
    "- Two stacked LSTM layers (32 and 16 units)\n",
    "- A hidden Dense layer (16 units)\n",
    "- A multi-output Dense layer (7 units)\n",
    "\n",
    "I also explored some variations including a single LSTM, varying numbers of Dense layers and varying numbers of units.\n",
    "Multiple experiments indicated that the stacked LSTM architecture with Dense layers demonstrate the best performance compared to other variations tested.\n",
    "\n",
    "#### Loss:\n",
    "I used Huber loss with an alpha parameter set to 0.25.  \n",
    "This choice helped the model focus on traffic patterns while disregarding occassional outliers encountered frequently.  \n",
    "The 0.25 parameter value was determined based on 3 times the median standard deviation of page visits to filter out these outliers.  \n",
    "\n",
    "*For further details refer to exploration.ipynb*\n",
    "\n",
    "#### Samples, epochs and batch size:\n",
    "The model is continuously supplied with random 90-day samples (followed by a 7-day predicted period) extracted from each page's visit history. Therefore a training epoch differs slightly from the conventional interpretation.\n",
    "During each epoch, the model trains using 'n' samples extracted from every page, ensuring all pages contribute to each epoch. Subsequent epochs involve new samples from different time periods, maintaining diversity in the training data.\n",
    "\n",
    "The number of samples per page, along with the batch size, determines whether batches comprise samples from the same or different pages:  \n",
    "- For instance, if BATCH_SIZE=16 and N_SAMPLES=1, each batch contains 16 inputs from distinct pages.\n",
    "- Conversely, if BATCH_SIZE=8 and N_SAMPLES=8, all 8 inputs in the batch are from a single page.\n",
    "\n",
    "I experimented with various batch sizes (from 8 to 128) and numbers of samples (from 1 to 8). Through hyperparameter tuning, n_samples=4 and batch_size=8 were identified as optimal values.  \n",
    "Reducing the batch size to 8 notably enhanced the model's generalization, albeit at the cost of increased computation time.  \n",
    "Simultaneously setting the number of samples to 4 enabled the model to capture page-specific patterns by having only two pages per batch with 4 samples each.  \n",
    "\n",
    "Efforts to equate the batch_size to the number of samples resulted in overfitting issues.\n",
    "\n",
    "#### Regularization:\n",
    "I conducted experiments using dropout and L2 regularization techniques.\n",
    "- L2 Regularization proved effective by directing the model to emphasize diverse features, aiding in learning complex patterns.\n",
    "- Dropout layers and LSTM's recurrent dropout didn't outperform L2 regularization and significantly increased the learning time. As a result, I chose to utilize L2 regularizatione exclusively.\n",
    "- In hyperparameter tuning kernel regularization hindered the model's learning process. In contrast, recurrent regularization yielded the most favorable outcomes. While bias regularization didn't significantly impact the model's performance, I decided to employ recurrent regularization for its consistent results.\n",
    "\n",
    "#### Optimizer and Learning Rate:\n",
    "I employed the standard Adam optimizer with a decaying learning rate strategy. This approach aimed to swiftly achieve satisfactory results and subsequently focus on capturing additional patterns.\n",
    "\n",
    "#### Hardware:\n",
    "Hyperparameter tuning and the final model optimization were performed using A100 GPUs, resulting in a 90% reduction in computation time compared to CPU-based calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from project_functions.sample_feed_v0_single import SampleFeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "TRAINING_WINDOW_SIZE = 90\n",
    "PREDICTED_WINDOW_SIZE = 7\n",
    "N_SAMPLES = 4\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_label = datetime.today().strftime(\"%m%d\")\n",
    "\n",
    "# Raw data\n",
    "\n",
    "features_train = dict(np.load(\"data/features_train.npz\", allow_pickle=True))\n",
    "features_valid = dict(np.load(\"data/features_valid.npz\", allow_pickle=True))\n",
    "\n",
    "# Calculated parameters\n",
    "\n",
    "n_rows_train = features_train['visits'].shape[0]\n",
    "n_features = features_train['time'].shape[1] + features_train['page'].shape[1] + 1\n",
    "\n",
    "steps_per_epoch = round(n_rows_train * N_SAMPLES / BATCH_SIZE)\n",
    "total_samples_per_page = N_SAMPLES * N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Feed\n",
    "\n",
    "sample_feed = SampleFeed(\n",
    "    training_window_size = TRAINING_WINDOW_SIZE,\n",
    "    predicted_window_size = PREDICTED_WINDOW_SIZE,\n",
    "    samples_per_epoch = N_SAMPLES,\n",
    "    n_features = n_features\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "Xy_train_gen = sample_feed.random_sample_stream(features_train)\n",
    "Xy_valid = sample_feed.random_sample_array(features_valid, samples_per_page=1, shuffle=False, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(TRAINING_WINDOW_SIZE, n_features)))\n",
    "model.add(layers.LSTM(\n",
    "    units=64, \n",
    "    return_sequences=True, \n",
    "    recurrent_regularizer=regularizers.L2(0.01)\n",
    "    ))\n",
    "model.add(layers.LSTM(\n",
    "    units=32, \n",
    "    return_sequences=False,\n",
    "    recurrent_regularizer=regularizers.L2(0.01)\n",
    "    ))\n",
    "model.add(layers.Dense(\n",
    "    units=16, \n",
    "    activation='relu'\n",
    "    ))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(PREDICTED_WINDOW_SIZE, 'sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.Huber(0.25), \n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "    metrics=metrics.RootMeanSquaredError()\n",
    "    )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model_callbacks = [\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=1e-5),\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=8),\n",
    "    callbacks.ModelCheckpoint(filepath=f\"models/checkpoints/{today_label}\" + \"{epoch:02d}-{val_root_mean_squared_error:.4f}.keras\", monitor='val_loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(\n",
    "    x = Xy_train_gen,\n",
    "    validation_data = Xy_valid,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs = N_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    callbacks = model_callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"models/best_{today_label}\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'], color='black')\n",
    "plt.plot(model_history.history['val_loss'], color='blue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
