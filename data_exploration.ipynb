{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "colors = {\n",
    "    'blue_light': (0.1, 0.3, 0.7, 0.7),\n",
    "    'blue': (0.1, 0.3, 0.7, 0.9),\n",
    "    'blue_strong': (0.1, 0.3, 0.7, 1.0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"input/train_2.csv\")\n",
    "df_wiki = df_wiki.set_index(\"Page\")\n",
    "\n",
    "df_wiki.columns = pd.to_datetime(df_wiki.columns)\n",
    "df_wiki = df_wiki.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checks for duplicated data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki.axes[0].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki.axes[1].duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing data:** \n",
    "\n",
    "A tolerance of 6% missing data is acceptable. Some of these gaps arise from instances where a wiki page records zero visits, which is normal.  \n",
    "Additionally, certain pages may show appearances or disappearances within the test time range, resulting in leading or trailing NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki.isna().mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki.loc[df_wiki.isna().any(axis=1), :][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data ratio gradually decreases over the examined period, with **strong fluctuations occuring in the last 2-3 months.**  \n",
    "The total visits display a gradual increase, corresponding to the decrease in missing data, with **significant peeks observed.**\n",
    "\n",
    "The fluctuations and peaks underwent further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfig, paxes = plt.subplots(2, 1, figsize=(12,6), sharex=True)\n",
    "\n",
    "df_wiki.isna().mean().plot(ax=paxes[0], title='Missing Data Ratio', color=colors['blue'])\n",
    "df_wiki.sum().plot(ax=paxes[1], title='Total visits', color=colors['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two significant dips in the missing data ratio observed on **2017-07-16** and **2017-07-25**.  \n",
    "However, the exceptionally high ratio of visited pages does not allign with any peek in total visits  \n",
    "Additionally, there is substantial **turbulence in the data throughout 2017-09**.  \n",
    "\n",
    "Given the complexity and difficulty in explaining these phenomena, the entire period from 2017-07-01 will be categorized as outliers and excluded from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfig, paxes = plt.subplots(1, 2, figsize=(12,3))\n",
    "\n",
    "df_wiki.loc[:, \"2017-04-01\":].isna().mean().plot(ax=paxes[0], color=colors['blue'])\n",
    "df_wiki.loc[:, \"2017-04-01\":].sum().plot(ax=paxes[1], color=colors['blue'])\n",
    "\n",
    "df_wiki.loc[:, \"2017-07-13\":\"2017-07-28\"].isna().agg([\"sum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several seasonal peaks and drops that appear to be normal, indicating seasonal patterns. However, one notably suspicious pattern stands out:  \n",
    "There is a substantial peak in Total and Max visits during 2016-07 and 2016-08 without a corresponding increase in median visits, suggesting the possibility of a small number of large outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfig, paxes = plt.subplots(3, 1, figsize=(12,9))\n",
    "\n",
    "df_wiki.sum().plot(ax=paxes[0], ylabel='Total visits', color=colors['blue'])\n",
    "df_wiki.median().plot(ax=paxes[1], ylabel='Median visits', color=colors['blue'])\n",
    "df_wiki.max().plot(ax=paxes[2], ylabel='Max visits', color=colors['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis regarding outliers in 2016-07 - 2016-08 seems accurate.  \n",
    "There are two records of Wikipedia's English main page showing unusually high values for precisely 1 month (from 2016-07-18 to 2016-08-18).  \n",
    "\n",
    "This discrepancy should be excluded from model training. Given the assumed significance of Wikipedia's English main page, instead of removing all the data for this records, I filled in the missing values using interpolation for this specific outlier period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_per_page = df_wiki.loc[:,\"2016-07-01\":\"2016-09-01\"].max(axis=1)\n",
    "\n",
    "outlier_pages = max_per_page[max_per_page >= 4e7].index\n",
    "\n",
    "df_wiki.loc[outlier_pages, \"2016-04-01\":\"2016-12-01\"].sum().plot(ylim=0, color=colors['blue_light'])\n",
    "df_wiki.loc[outlier_pages, \"2016-07-18\":\"2016-08-18\"].sum().plot(ylim=0, color=colors['blue_strong'])\n",
    "plt.title(f\"Number of outlier pages: {len(outlier_pages)}\\n{outlier_pages[0]}\\n{outlier_pages[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine distribution**\n",
    "\n",
    "The visits data exhibits an extremely skewed distribution, making it barely visible when plotted on a histogram using a linear scale.\n",
    "To address this issue, we apply a logarithmic transformation using the *log(x+1)* function.\n",
    "This transformation helps the model in emphasizing detailed patterns and interpreting peaks and drops in terms of their relative magnitudes.  \n",
    "\n",
    "The median visits display substantial variations among different groups (as depicted below). \n",
    "This observation shows that the model needs to forecast traffic for various types of pages, such as smaller obscure pages, intermittently popular pages, extensively visited pages, and exceptionally large pages like the main page. \n",
    "To capture the diverse behaviors among these distinct classes, I will utilize the normalized logarithm of median visits as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_median = df_wiki.median(axis=1)\n",
    "wiki_median_log = wiki_median.apply(np.log1p)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs[0].suptitle(\"Distribution of pages' visits median\", fontweight='bold')\n",
    "\n",
    "axs0 = subfigs[0].subplots(1, 2)\n",
    "\n",
    "wiki_median.plot.hist(bins=200, ax=axs0[0])\n",
    "wiki_median.plot.box(ax=axs0[1], vert=False, label='')\n",
    "\n",
    "axs1 = subfigs[1].subplots(1, 2)\n",
    "\n",
    "subfigs[1].suptitle(\"Distribution of pages' visits median (logarithmic)\", fontweight='bold')\n",
    "\n",
    "wiki_median_log.plot.hist(bins=200, ax=axs1[0])\n",
    "wiki_median_log.plot.box(ax=axs1[1], vert=False, label='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data normalization**\n",
    "\n",
    "To improve the learning process visits data was scaled.\n",
    "I used a simple normalization by a maximum value separate for each page:\n",
    "- to train the model based on values in a range of (0,1)\n",
    "- so each page has similar impact on the loss function, regardless of its traffic magnitude\n",
    "- to not shift the data (logarithmic function was already applied)\n",
    "- as we assume that 0 is a minimum traffic value for each page (there's no point in deriving minimum value from data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_functions.normalization import MaxAbsLogScaler\n",
    "\n",
    "scaler = MaxAbsLogScaler()\n",
    "\n",
    "i_page = df_wiki.iloc[0, :].to_numpy().reshape(1, -1)\n",
    "i_page_scaled = scaler.fit_transform(i_page)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(i_page[0, :])\n",
    "ax[0].set_title('Visits raw', fontweight='bold')\n",
    "ax[0].set_ylim(0)\n",
    "\n",
    "ax[1].plot(i_page_scaled[0, :])\n",
    "ax[1].set_title('Visits (normalized logarithm)', fontweight='bold')\n",
    "ax[1].set_ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "Despite the transformations mentioned earlier, certain significant peeks remain unpredictable due to their unknown nature.  \n",
    "To mitigate their influence during training, I applied Huber loss function.\n",
    "\n",
    "The Delta parameter for Huber Loss is determined through the following steps:\n",
    "- Compute the logarithm of normalized visits.\n",
    "- Calculate the standard deviation for each page.\n",
    "- Derive the median standard deviation across all pages.\n",
    "- Set Delta as 3 times the calculated median standard deviation (rule of thumb for the outlier treshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_scaler = MaxAbsLogScaler()\n",
    "\n",
    "all_pages = df_wiki.to_numpy()\n",
    "all_pages_scaled = multi_scaler.fit_transform(all_pages)\n",
    "\n",
    "stds = np.nanstd(all_pages_scaled, axis=1) \n",
    "plt.hist(stds, bins=50)\n",
    "plt.title('Standard deviation')\n",
    "\n",
    "np.median(stds) * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for seasonality:**  \n",
    "\n",
    "I conducted an autocorrelation analysis on the number of visits across all pages to identify any seasonal patterns. Two distinct seasonal patterns were observed:\n",
    "- weekly\n",
    "- yearly\n",
    "\n",
    "To enhance the model's capability in capturing seasonal patterns for predictions, I included additional time features including:\n",
    "- Sine and cosine waves with periods of 7 and 365 days\n",
    "- Autocorrelation of page visits lagged with intervals of 7 and 365 days\n",
    "\n",
    "Finally, due to the limited duration of available data (only about 2 years), only the weekly seasonality was incorporated into the model. Modeling yearly seasonality with such a short period was unfeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "autocorrs = np.apply_along_axis(\n",
    "    func1d=lambda arr, nlags: sm.tsa.acf(x=arr, nlags=nlags, missing='conservative'), \n",
    "    axis=1,    \n",
    "    arr=all_pages_scaled, \n",
    "    nlags=390\n",
    "    )\n",
    "\n",
    "avg_autocorrs = autocorrs.mean(axis=0, where=~np.isnan(autocorrs))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16, 4))\n",
    "\n",
    "ax[0].plot(avg_autocorrs[0:25])\n",
    "ax[0].set_xticks(range(0, 25, 7))\n",
    "ax[0].set_title(\"Weekly\")\n",
    "\n",
    "ax[1].plot(avg_autocorrs)\n",
    "ax[1].set_xticks(range(0, 391, 365))\n",
    "ax[1].set_title(\"Yearly\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploration of page information**\n",
    "\n",
    "As per the problem's description:  \n",
    "*\"The page names contain the Wikipedia project (e.g. en.wikipedia.org), type of access (e.g. desktop) and type of agent (e.g. spider).  \n",
    "In other words, each article name has the following format: 'name_project_access_agent' (e.g. 'AKB48_zh.wikipedia.org_all-access_spider').\"*\n",
    "\n",
    "Regarding these features:\n",
    "- The features were extracted and analyzed to assess their utility for the model.\n",
    "- The project, access, and agent features were included in the model as they represent factor variables with a few levels, making them suitable for OneHotEncoding.\n",
    "- The page names were excluded from the model due to the high volume of unique values (49k out of 145k rows), making their incorporation challenging. While page names under different agents or access types might exhibit common behavior, the limited data available for each level (usually 2-3 rows) raises concerns about potential overfitting. Therefore, the usefulness of this feature remains questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_groups = [\n",
    "    r'(?P<page_name>^.*(?=_))'\n",
    "    r'(?P<junk_1>_)',\n",
    "    r'(?P<project>(?<=_)[^_]*(?=_))', \n",
    "    r'(?P<junk_2>_)',\n",
    "    r'(?P<access>(?<=_)[^_]*(?=_))', \n",
    "    r'(?P<junk_3>_)', \n",
    "    r'(?P<agent>(?<=_).*$)'\n",
    "    ]\n",
    "\n",
    "page_features = df_wiki.index. \\\n",
    "    str.extract(\"\".join(extract_groups)). \\\n",
    "    drop(columns=['junk_1', 'junk_2', 'junk_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_features.describe().iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\\n------------------------------\\n\",\n",
    "    page_features.project.value_counts(), \n",
    "    \"\\n------------------------------\\n\",\n",
    "    page_features.access.value_counts(), \n",
    "    \"\\n------------------------------\\n\", \n",
    "    page_features.agent.value_counts(),\n",
    "    \"\\n------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "1. Exclude the Main Page data between 2016-07-18 and 2016-08-18\n",
    "2. Exclude the entire period starting from 2017-07-01\n",
    "3. Apply a logarithmic function and normalize visits data using the maximum absolute value.\n",
    "4. Use Huber loss function with a delta value of 0.25.\n",
    "5. Utilize the normalized logarithm of page median visits as a feature representing the magnitude of traffic for each page.\n",
    "6. Incorporate additional time features, represented by sine and cosine waves with period of 7 and autocorrelation of page visits lagged with interval of 7 days.\n",
    "7. Include additional one-hot-encoded page features: project, access and agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
